{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0235f845",
   "metadata": {},
   "source": [
    "## Creiamo un metodo per leggere il file.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9148807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pprint\n",
    "def read_csv(file_name):\n",
    "    meanings_map = {}\n",
    "    dir = os.getcwd()\n",
    "    with open(dir + '/' + file_name, \"r\") as csv_file:\n",
    "        words = csv_file.readline().replace('\\n','').split(',')\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            for i in range(1, 5):\n",
    "                if not meanings_map.keys().__contains__(words[i]):\n",
    "                    meanings_map[words[i]] = []\n",
    "                meanings_map[words[i]].append(row[i]) if len(row[i]) > 0 else None\n",
    "    return meanings_map\n",
    "\n",
    "# Esempio output\n",
    "#pprint.pprint(read_csv('defs.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc52fc",
   "metadata": {},
   "source": [
    "## Creiamo una dict con l'elenco delle parole e la loro frequenza\n",
    "Dal risultato di `read_csv('defs.csv')` otteniamo una dict dove per ogni termine abbiamo una lista di definizioni. \n",
    "\n",
    "Il risultato finale di questo step sarà una dictionary dove per ogni parola avremo una dict termine : frequenza, questa sarà usata per la ricerca del synset su wordnet tramite il principio del Genus\n",
    "\n",
    "Per farlo \n",
    "\n",
    "1. Concateniamo tutte le definizioni in una unica stringa.\n",
    "2. Creiamo una lista di token dell'intera lista\n",
    "3. filtriamo le parole che sono alfanumerici ( per rimuovere punteggiatura) e che non siano stop_words.\n",
    "\n",
    "Infine memorizziamo il lemma del termine e calcoliamo la frequenza di ogni parola. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dac6e89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Apprehension': {'abnormal': 1,\n",
      "                  'accada': 1,\n",
      "                  'act': 1,\n",
      "                  'agitated': 1,\n",
      "                  'agitation': 2,\n",
      "                  'ansia': 1,\n",
      "                  'anticipation': 1,\n",
      "                  'anxiety': 10,\n",
      "                  'awe': 1,\n",
      "                  'bad': 4,\n",
      "                  'brutto': 1,\n",
      "                  'cause': 1,\n",
      "                  'characterized': 1,\n",
      "                  'che': 1,\n",
      "                  'constant': 1,\n",
      "                  'derived': 1,\n",
      "                  'di': 1,\n",
      "                  'different': 1,\n",
      "                  'discomfort': 1,\n",
      "                  'disturbance': 1,\n",
      "                  'emotion': 2,\n",
      "                  'emotional': 1,\n",
      "                  'event': 1,\n",
      "                  'expectation': 1,\n",
      "                  'experience': 1,\n",
      "                  'fear': 10,\n",
      "                  'fearful': 1,\n",
      "                  'feel': 3,\n",
      "                  'feeling': 6,\n",
      "                  'frightened': 1,\n",
      "                  'future': 1,\n",
      "                  'happen': 5,\n",
      "                  'high': 1,\n",
      "                  'learning': 1,\n",
      "                  'loss': 1,\n",
      "                  'make': 2,\n",
      "                  'mental': 3,\n",
      "                  'mind': 3,\n",
      "                  'mood': 1,\n",
      "                  'moode': 1,\n",
      "                  'negative': 1,\n",
      "                  'nervousness': 1,\n",
      "                  'normal': 1,\n",
      "                  'one': 2,\n",
      "                  'particular': 1,\n",
      "                  'pathway': 1,\n",
      "                  'paura': 1,\n",
      "                  'person': 3,\n",
      "                  'preoccupation': 1,\n",
      "                  'qualcosa': 1,\n",
      "                  'restless': 1,\n",
      "                  'sadness': 1,\n",
      "                  'sense': 1,\n",
      "                  'situation': 2,\n",
      "                  'someone': 1,\n",
      "                  'something': 10,\n",
      "                  'spiacevole': 1,\n",
      "                  'state': 7,\n",
      "                  'status': 2,\n",
      "                  'strange': 2,\n",
      "                  'strangeness': 1,\n",
      "                  'unaccommodating': 1,\n",
      "                  'uncofortable': 1,\n",
      "                  'understanding': 1,\n",
      "                  'understood': 1,\n",
      "                  'unease': 1,\n",
      "                  'unexpected': 1,\n",
      "                  'unpleasant': 3,\n",
      "                  'upset': 1,\n",
      "                  'way': 1,\n",
      "                  'worry': 1},\n",
      " 'Courage': {'abiliity': 1,\n",
      "             'ability': 18,\n",
      "             'able': 3,\n",
      "             'act': 1,\n",
      "             'action': 2,\n",
      "             'allow': 1,\n",
      "             'allows': 4,\n",
      "             'aptitude': 1,\n",
      "             'avoid': 1,\n",
      "             'behavior': 1,\n",
      "             'beyond': 1,\n",
      "             'blocked': 1,\n",
      "             'characteristic': 1,\n",
      "             'choice': 2,\n",
      "             'considered': 1,\n",
      "             'control': 2,\n",
      "             'danger': 1,\n",
      "             'dangerous': 3,\n",
      "             'deal': 1,\n",
      "             'despite': 3,\n",
      "             'difficult': 3,\n",
      "             'difficulty': 1,\n",
      "             'drastic': 1,\n",
      "             'emotion': 1,\n",
      "             'expectation': 1,\n",
      "             'face': 9,\n",
      "             'fear': 17,\n",
      "             'fearful': 1,\n",
      "             'feeling': 2,\n",
      "             'frightened': 1,\n",
      "             'generally': 1,\n",
      "             'go': 1,\n",
      "             'hero': 1,\n",
      "             'human': 1,\n",
      "             'inner': 1,\n",
      "             'make': 4,\n",
      "             'may': 1,\n",
      "             'mental': 1,\n",
      "             'mind': 1,\n",
      "             'moral': 1,\n",
      "             'one': 1,\n",
      "             'overcome': 3,\n",
      "             'particular': 1,\n",
      "             'people': 2,\n",
      "             'perform': 1,\n",
      "             'persevere': 1,\n",
      "             'person': 1,\n",
      "             'property': 1,\n",
      "             'provoke': 1,\n",
      "             'quality': 1,\n",
      "             'resist': 1,\n",
      "             'risk': 1,\n",
      "             'risky': 1,\n",
      "             'scar': 1,\n",
      "             'scare': 1,\n",
      "             'scared': 2,\n",
      "             'scaring': 1,\n",
      "             'scary': 1,\n",
      "             'situation': 7,\n",
      "             'someone': 1,\n",
      "             'something': 7,\n",
      "             'strength': 3,\n",
      "             'take': 2,\n",
      "             'taking': 1,\n",
      "             'thaht': 1,\n",
      "             'thing': 2,\n",
      "             'typical': 1,\n",
      "             'u': 3,\n",
      "             'unpleasant': 1,\n",
      "             'unpleasent': 1,\n",
      "             'venture': 1,\n",
      "             'willing': 1,\n",
      "             'without': 2,\n",
      "             'withstand': 1},\n",
      " 'Paper': {'available': 1,\n",
      "           'cellulose': 7,\n",
      "           'communication': 1,\n",
      "           'composed': 2,\n",
      "           'context': 1,\n",
      "           'cortex': 1,\n",
      "           'crafted': 1,\n",
      "           'cut': 1,\n",
      "           'derived': 2,\n",
      "           'done': 1,\n",
      "           'draw': 1,\n",
      "           'drawing': 1,\n",
      "           'easily': 1,\n",
      "           'easy': 1,\n",
      "           'especially': 1,\n",
      "           'felted': 1,\n",
      "           'fiber': 1,\n",
      "           'fine': 1,\n",
      "           'flat': 1,\n",
      "           'folded': 1,\n",
      "           'generally': 2,\n",
      "           'handwriting': 1,\n",
      "           'information': 1,\n",
      "           'kind': 1,\n",
      "           'laid': 1,\n",
      "           'lightweight': 1,\n",
      "           'made': 4,\n",
      "           'material': 23,\n",
      "           'medium': 1,\n",
      "           'multiple': 1,\n",
      "           'note': 1,\n",
      "           'object': 1,\n",
      "           'obtained': 5,\n",
      "           'one': 2,\n",
      "           'packaging': 1,\n",
      "           'particular': 1,\n",
      "           'piece': 1,\n",
      "           'possible': 1,\n",
      "           'printing': 2,\n",
      "           'product': 3,\n",
      "           'purpose': 1,\n",
      "           'retrieve': 1,\n",
      "           'rip': 1,\n",
      "           'screen': 1,\n",
      "           'several': 1,\n",
      "           'sheet': 1,\n",
      "           'short': 1,\n",
      "           'something': 1,\n",
      "           'store': 1,\n",
      "           'student': 1,\n",
      "           'subject': 1,\n",
      "           'surface': 1,\n",
      "           'suspension': 1,\n",
      "           'taking': 1,\n",
      "           'thin': 2,\n",
      "           'tree': 5,\n",
      "           'type': 1,\n",
      "           'university': 1,\n",
      "           'used': 13,\n",
      "           'usually': 1,\n",
      "           'vegetable': 1,\n",
      "           'water': 1,\n",
      "           'wood': 6,\n",
      "           'write': 8,\n",
      "           'writing': 8,\n",
      "           'written': 2},\n",
      " 'Sharpener': {'allow': 1,\n",
      "               'allows': 3,\n",
      "               'blade': 2,\n",
      "               'clearer': 1,\n",
      "               'creating': 1,\n",
      "               'cut': 1,\n",
      "               'device': 2,\n",
      "               'edge': 1,\n",
      "               'end': 1,\n",
      "               'equipped': 1,\n",
      "               'graphite': 1,\n",
      "               'lead': 1,\n",
      "               'little': 1,\n",
      "               'make': 4,\n",
      "               'making': 2,\n",
      "               'mark': 1,\n",
      "               'mine': 2,\n",
      "               'object': 12,\n",
      "               'order': 1,\n",
      "               'pencil': 25,\n",
      "               'person': 2,\n",
      "               'refined': 1,\n",
      "               'returning': 1,\n",
      "               'shapen': 1,\n",
      "               'sharp': 4,\n",
      "               'sharpen': 16,\n",
      "               'sharpening': 2,\n",
      "               'sharper': 1,\n",
      "               'sharpner': 1,\n",
      "               'smooth': 1,\n",
      "               'something': 3,\n",
      "               'stationery': 1,\n",
      "               'tip': 2,\n",
      "               'tool': 16,\n",
      "               'use': 2,\n",
      "               'used': 15,\n",
      "               'working': 1,\n",
      "               'write': 2}}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "\n",
    "meanings_map = read_csv('defs.csv')\n",
    "lm = WordNetLemmatizer()\n",
    "#porter = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "counter_map = {}\n",
    "for word in meanings_map.keys():\n",
    "    definitions_join = ' '.join(meanings_map.get(word))\n",
    "    word_tokens = word_tokenize(definitions_join)\n",
    "    list_definitio_word = [lm.lemmatize(w.lower()) for w in word_tokens if not w.lower() in stop_words and w.lower().isalnum()]\n",
    "    #list_definitio_word = [porter.stem(w.lower()) for w in word_tokens if not w.lower() in stop_words and w.lower().isalnum()]\n",
    "    counter_map[word] = dict(Counter(list_definitio_word))\n",
    "\n",
    "# Esempio\n",
    "pprint.pprint(counter_map)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c03bd5",
   "metadata": {},
   "source": [
    "## Creiamo un metodo per semplificare l'estrazione delle k word più frequenti data una parola\n",
    "Questo ci servirà per applicare il principio del Genus\n",
    "\n",
    "Per farlo sfruttiamo il metodo sorted dove passiamo la dictionary, specifichiamo la key tramite lambda calcolo e settiamo la modalità reverse per ottenere in ordine decrescente. \n",
    "\n",
    "Al temrine estriamo solo le prime K parole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "860a5174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le prime 10 parole più frequenti per Apprehension sono:\n",
      "['something', 'fear', 'anxiety', 'state', 'feeling', 'happen', 'bad', 'feel', 'mind', 'mental']\n"
     ]
    }
   ],
   "source": [
    "def get_most_frequent_words(term, k= 99999):\n",
    "    return [k for k, v in sorted(counter_map.get(term).items(), key=lambda item: item[1], reverse=True)][0:k]\n",
    "\n",
    "# Esempio\n",
    "print(\"Le prime 10 parole più frequenti per Apprehension sono:\")\n",
    "print(get_most_frequent_words('Apprehension', 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfcd8a0",
   "metadata": {},
   "source": [
    "# Definiamo ora una serie di metodi per calcolare l'overlap\n",
    "Ci serve un sistema per valutare i nostri synset per capire quale potrebbe essere quello più corretto tra gli iponimi dei nostri\n",
    "K termini estratti precedentemente.\n",
    "\n",
    "## Metodo bag of word\n",
    "\n",
    "Il primo metodo `bag_of_words_for_synset(wn_syn)` dato un synset estrae una bag of word con le parole contenute negli esempi \n",
    "e definizioni. Il metodo di filtraggio è lo stesso dello step precedente.\n",
    "\n",
    "Per semplificare non son stati eseguiti conteggi sulle parole ripetute anche se avrebbe potuto portare ad una maggiore accuratezza durante la fase di overlapping.\n",
    "\n",
    "Il risultato è un set delle parole estratte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6effc379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dog': 1, 'barked': 1, 'night': 1, 'member': 10, 'genus': 10, 'canis': 10, 'probably': 10, 'descended': 10, 'common': 10, 'wolf': 10, 'domesticated': 10, 'man': 10, 'since': 10, 'prehistoric': 10, 'time': 10, 'occurs': 10, 'many': 10, 'breed': 10}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def bag_of_words_for_synset(wn_syn):\n",
    "    new_list = wn_syn.examples() + [wn_syn.definition()]\n",
    "    sentence_concatenated = ' '.join(new_list)\n",
    "    tokens = word_tokenize(sentence_concatenated)\n",
    "    bag_of_words = [lm.lemmatize(w.lower()) for w in tokens if not w.lower() in stop_words and w.lower().isalnum()]\n",
    "    #bag_of_words = [porter.stem(w.lower()) for w in tokens if not w.lower() in stop_words and w.lower().isalnum()]\n",
    "    return dict(Counter(bag_of_words))\n",
    "\n",
    "# Esempio\n",
    "print(bag_of_words_for_synset(wn.synset('dog.n.01')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d05ef9",
   "metadata": {},
   "source": [
    "## Metodo overlap\n",
    "\n",
    "Creiamo ora un metodo per calcolare l'overlap date due dict di bag of words, rispettivamente del synset e delle definizioni di partenza\n",
    "\n",
    "Questo metodo prese due dict restituisce il numero di intersezioni, dove un'intersezione avviene se due parole han lo stesso stemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "30581240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "def compute_overlap(dict_a, dict_b):\n",
    "    return sum(dict(Counter(dict_a) & Counter(dict_b)).values())\n",
    "\n",
    "# Esempio\n",
    "print(compute_overlap(counter_map.get('Apprehension'), bag_of_words_for_synset(wn.synset('apprehension.n.01'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bcb4ed",
   "metadata": {},
   "source": [
    "## Per WSD -> Lesk\n",
    "Per disambiguare sarà usato l'algoritmo lesk fornito dalle librerie nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4f857e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('fear.n.03')\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "\n",
    "# lesk (lista parole della frase, parola da disambiguare)\n",
    "print(lesk((get_most_frequent_words('Apprehension')), 'fear','n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e665d5d4",
   "metadata": {},
   "source": [
    "## Cerchiamo il synset data la definizione\n",
    "\n",
    "Per ogni termine da cercare estriamo le 10 più frequenti parole usando \n",
    "```get_most_frequent_words(word,10)```\n",
    "Per ognuna di esse disambiguiamo usando lesk di nltk come da esempio precedente.\n",
    "Nota: in questo caso non è passato nessun K perchè preferiamo estrarre tutte le parole delle definizioni per maggiore accuratezza\n",
    "\n",
    "A questo punto per ogni iponimo di queste 10 parole calcoliamo l'overlap sul synset disambiguato.\n",
    "\n",
    "Il risultato sarà salvato in una dictionary nuova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2539b758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courage\n",
      "[(Synset('physical_ability.n.01'), 5),\n",
      " (Synset('penetration.n.04'), 3),\n",
      " (Synset('stage_fright.n.01'), 3),\n",
      " (Synset('countenance.n.03'), 3),\n",
      " (Synset('take_the_bull_by_the_horns.v.01'), 3)]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Paper\n",
      "[(Synset('composite_material.n.01'), 5),\n",
      " (Synset('paper.n.01'), 5),\n",
      " (Synset('packing_material.n.01'), 4),\n",
      " (Synset('literature.n.03'), 4),\n",
      " (Synset('jot_down.v.01'), 4)]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Apprehension\n",
      "[(Synset('feeling.n.01'), 5),\n",
      " (Synset('condition.n.01'), 4),\n",
      " (Synset('grace.n.01'), 4),\n",
      " (Synset('situation.n.01'), 4),\n",
      " (Synset('apprehension.n.01'), 3)]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sharpener\n",
      "[(Synset('drill.n.01'), 4),\n",
      " (Synset('acuminate.v.01'), 4),\n",
      " (Synset('jaws_of_life.n.01'), 3),\n",
      " (Synset('plow.n.01'), 3),\n",
      " (Synset('upset.n.04'), 3)]\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_dict = {}\n",
    "for word in counter_map.keys():\n",
    "    #print(word)\n",
    "    top_frequent_terms = get_most_frequent_words(word,4)\n",
    "    inner_dict = {}\n",
    "    for f_word in top_frequent_terms:\n",
    "        #synset = lesk(get_most_frequent_words(word), f_word, 'n')\n",
    "        for synset in wn.synsets(f_word):\n",
    "            #print(synset)\n",
    "            if synset is not None:\n",
    "                for hyp in synset.hyponyms():\n",
    "                    #print(\"\\t\", hyp)\n",
    "                    if not inner_dict.__contains__(hyp):\n",
    "                        bag_of_word_hyp = bag_of_words_for_synset(hyp)\n",
    "                        overlap = compute_overlap(counter_map.get(word), bag_of_word_hyp)\n",
    "                        inner_dict[hyp] = overlap\n",
    "    #print(\"\\n\\n\")\n",
    "    result_dict[word] = inner_dict\n",
    "\n",
    "for word in result_dict.keys():\n",
    "    print(word)\n",
    "    pprint.pprint(sorted(result_dict.get(word).items(), key=lambda item: item[1], reverse=True)[0:5])\n",
    "    print(\"\\n\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2912f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
